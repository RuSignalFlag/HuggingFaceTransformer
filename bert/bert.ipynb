{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12290,"status":"ok","timestamp":1684943550409,"user":{"displayName":"shuaiqi duan","userId":"08897309869639517662"},"user_tz":-480},"id":"ZZw0i29js7Rx","outputId":"e66b2fee-3432-4856-f244-cab387207898"},"outputs":[],"source":["!pip3 install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip3 install d2l"]},{"cell_type":"code","execution_count":73,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":5014,"status":"ok","timestamp":1684943782004,"user":{"displayName":"shuaiqi duan","userId":"08897309869639517662"},"user_tz":-480},"id":"_9DbGm-zWMwz","outputId":"22d6a375-8317-4e63-bb31-37973bc86204"},"outputs":[{"data":{"text/plain":["'cuda'"]},"execution_count":73,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","import torch.nn as nn\n","import os \n","import random\n","from d2l import torch as d2l\n","from torch.utils.data import Dataset,DataLoader\n","from typing import List, Optional, Tuple, Union\n","import math\n","from torch._C import dtype\n","import time\n","import matplotlib.pyplot as plt\n","from IPython import display\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","device"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"BzN4XNIPyseU"},"source":["# Datasets"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":1421,"status":"ok","timestamp":1684943787262,"user":{"displayName":"shuaiqi duan","userId":"08897309869639517662"},"user_tz":-480},"id":"v73-luttxC-l"},"outputs":[],"source":["d2l.DATA_HUB['wikitext-2'] = (\n","    'https://s3.amazonaws.com/research.metamind.io/wikitext/'\n","    'wikitext-2-v1.zip', '3c914d17d80b1459be871a5039ac23e752a53cbe')"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1684943789329,"user":{"displayName":"shuaiqi duan","userId":"08897309869639517662"},"user_tz":-480},"id":"EbU5PsCWyp4d"},"outputs":[],"source":["def _read_wiki(data_dir):\n","  file_name = os.path.join(data_dir, 'wiki.train.tokens')\n","  with open(file_name, 'r') as f:\n","    lines = f.readlines()\n","  paragraphs = [line.strip().lower().split(' . ')\n","                for line in lines if len(line.split(' . ')) >= 2]\n","  random.shuffle(paragraphs)\n","  return paragraphs\n","  "]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"8P2GDxJr6Ihh"},"source":["## next sentence Predict task"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1684943792014,"user":{"displayName":"shuaiqi duan","userId":"08897309869639517662"},"user_tz":-480},"id":"lhXt1d_vzyT6"},"outputs":[],"source":["def _get_next_sentence(sentence, next_sentence, paragraphs):\n","  if random.random()<0.5:\n","    is_next = True\n","  else:\n","    # paragraphs是三重列表的嵌套\n","    next_sentence = random.choice(random.choice(paragraphs))\n","    is_next = False\n","  return sentence, next_sentence, is_next"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1684943794368,"user":{"displayName":"shuaiqi duan","userId":"08897309869639517662"},"user_tz":-480},"id":"UfGD3li46FcQ"},"outputs":[],"source":["def get_tokens_and_segments(tokens_a, tokens_b=None):\n","  \"\"\"\n","  获取输入序列的词元及其段索引\n","  \"\"\"\n","  tokens = ['<cls>'] + tokens_a + ['<sep>']\n","  #0和1分别表示标记段A和B\n","  segments = [0]* (len(tokens_a) + 2)\n","  if tokens_b is not None:\n","    tokens += tokens_b + ['<sep>']\n","    segments += [1]* (len(tokens_b) + 1 )\n","  return tokens, segments"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1684943796258,"user":{"displayName":"shuaiqi duan","userId":"08897309869639517662"},"user_tz":-480},"id":"1Xve7Lu13nAS"},"outputs":[],"source":["def  _get_nsp_data_from_paragraph(paragraph, paragraphs, vocab, max_len):\n","  nsp_data_from_paragraph = []\n","  for i in range(len(paragraph)-1):\n","    tokens_a, tokens_b, is_next = _get_next_sentence(paragraph[i], paragraph[i+1],paragraphs)\n","    #考虑一个'<cls>'词和两个'<sep>'词\n","    if len(tokens_a) + len(tokens_b) + 3 > max_len:\n","      continue\n","    tokens, segments = get_tokens_and_segments(tokens_a, tokens_b)\n","    nsp_data_from_paragraph.append((tokens, segments, is_next))\n","  return nsp_data_from_paragraph"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"o5GLaRT77-XB"},"source":["## mask language model task"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1684943798591,"user":{"displayName":"shuaiqi duan","userId":"08897309869639517662"},"user_tz":-480},"id":"8OIrkxyj8Atj"},"outputs":[],"source":["def _replace_mlm_tokens(tokens, candiate_pred_positions, num_mlm_preds, vocab):\n","  #为掩码语言模型的输入创建新的词元副本，其中输入可能包含替换的'<mask>'词元或随机词元\n","  mlm_input_tokens = [token for token in tokens]\n","  pred_positions_and_labes = []\n","  #打乱后用于在掩码语言模型中获取15%的随即词元进行预测\n","  random.shuffle(candiate_pred_positions)\n","  for mlm_pred_position in candiate_pred_positions:\n","    if len(pred_positions_and_labes) >= num_mlm_preds:\n","      break\n","    masked_token = None\n","    #80%的时间，将词元替换为'<mask>'词元\n","    if random.random() < 0.8:\n","      masked_token = '<mask>'\n","    else:\n","      #10%的概率保持词元不变\n","      if random.random() < 0.5:\n","        masked_token = tokens[mlm_pred_position]\n","      else:\n","        #10%概率，用随机词元替换\n","        masked_token = random.choice(vocab.idx_to_token)\n","    mlm_input_tokens[mlm_pred_position] = masked_token\n","    pred_positions_and_labes.append((mlm_pred_position, tokens[mlm_pred_position]))\n","  return mlm_input_tokens, pred_positions_and_labes"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1684943800362,"user":{"displayName":"shuaiqi duan","userId":"08897309869639517662"},"user_tz":-480},"id":"vmcmtwDsANWx"},"outputs":[],"source":["def _get_mlm_data_from_tokens(tokens,vocab):\n","  candiate_pred_positions = []\n","  #tokens是一个字符串列表\n","  for i, token in enumerate(tokens):\n","    #剔除掉特殊词元\n","    if token in ['<cls>', '<sep>']:\n","      continue\n","    candiate_pred_positions.append(i)\n","  #预测15%得随机词元\n","  num_mlm_preds = max(1, round(len(candiate_pred_positions) * 0.15))\n","  mlm_input_tokens, pred_positions_and_labes = _replace_mlm_tokens(tokens, candiate_pred_positions,\n","                                                                   num_mlm_preds, vocab)\n","  pred_positions_and_labes = sorted(pred_positions_and_labes,\n","                                    key = lambda x: x[0])\n","  pred_positions = [v[0] for v in pred_positions_and_labes]\n","  mlm_pred_labels = [v[1] for v in pred_positions_and_labes]\n","  return vocab[mlm_input_tokens], pred_positions, vocab[mlm_pred_labels]\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"2fsETwcQFUSX"},"source":["## build datasets"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":1145,"status":"ok","timestamp":1684943803750,"user":{"displayName":"shuaiqi duan","userId":"08897309869639517662"},"user_tz":-480},"id":"jQclm-OjFWWe"},"outputs":[],"source":["def _pad_bert_inputs(examples, max_len, vocab):\n","  max_num_mlm_preds = round(max_len*0.15)\n","  all_tokens_ids, all_segments, attention_mask = [], [], []\n","  all_pred_positions, all_mlm_weights, all_mlm_labels = [], [], []\n","  nsp_labels = []\n","  for (token_ids, pred_positions, mlm_pres_label_ids, segments, is_next) in examples:\n","    all_tokens_ids.append(torch.tensor(token_ids +  [vocab['<pad>']] * (max_len - len(token_ids)),dtype=torch.long).to(device))\n","    all_segments.append(torch.tensor(segments + [0] * (max_len - len(segments)),dtype=torch.long).to(device))\n","    #attention_mask不包括pad的计数\n","    attention_mask.append(torch.tensor([1] *len(token_ids) + [0] * (max_len - len(token_ids)),dtype=torch.float32).to(device))\n","    all_pred_positions.append(torch.tensor(pred_positions + [0] * (max_num_mlm_preds - len(pred_positions)),dtype=torch.long).to(device))\n","    #填充词元的预测将通过乘以权重seita在损失中过滤掉\n","    all_mlm_weights.append(torch.tensor([1.0] * len(pred_positions) + [0.0] * (max_num_mlm_preds - len(pred_positions)),dtype=torch.float32).to(device))\n","    all_mlm_labels.append(torch.tensor(mlm_pres_label_ids +[0]*(max_num_mlm_preds - len(mlm_pres_label_ids)), dtype=torch.long).to(device))\n","    nsp_labels.append(torch.tensor(is_next, dtype=torch.long).to(device))\n","  return (all_tokens_ids, all_segments, attention_mask, all_pred_positions, all_mlm_weights, all_mlm_labels, nsp_labels)"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1684943805953,"user":{"displayName":"shuaiqi duan","userId":"08897309869639517662"},"user_tz":-480},"id":"vmVGzWpkPOV_"},"outputs":[],"source":["class _WikiTextDataset(Dataset):\n","  def __init__(self, paragraphs, max_len):\n","    super().__init__()\n","    \"\"\"\n","    输入paragraphs[i]是代表段落的句子字符串列表\n","    而输出paragraphs[i]是代表段落的句子列表，其中每个句子都是词元列表\n","    \"\"\"\n","    paragraphs = [d2l.tokenize(paragraph, token = 'word') for paragraph in paragraphs]\n","    sentences = [sentence for paragraph in paragraphs for sentence in paragraph]\n","    self.vocab = d2l.Vocab(sentences, min_freq=5, reserved_tokens=['<pab>', '<mask>', '<cls>', '<sep>'])\n","    #获取下一句预测任务的数据\n","    examples = []\n","    for paragraph in paragraphs:\n","      examples.extend(_get_nsp_data_from_paragraph(paragraph,paragraphs,self.vocab,max_len))\n","    #获取掩码语言模型的任务的数据\n","    examples = [(_get_mlm_data_from_tokens(tokens, self.vocab) + (segments, is_next)) for tokens,segments,is_next in examples]\n","    #填充输入\n","    (self.all_tokens_ids, self.all_segments, self.attention_mask, self.all_pred_positions, self.all_mlm_weights, self.all_mlm_labels,\n","     self.nsp_labels) = _pad_bert_inputs(examples,max_len,self.vocab)\n","\n","  def __getitem__(self, idx):\n","    return (self.all_tokens_ids[idx], self.all_segments[idx], self.attention_mask[idx], self.all_pred_positions[idx], self.all_mlm_weights[idx], self.all_mlm_labels[idx],\n","     self.nsp_labels[idx])\n","    \n","  def __len__(self):\n","    return len(self.all_tokens_ids)  "]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1684943808110,"user":{"displayName":"shuaiqi duan","userId":"08897309869639517662"},"user_tz":-480},"id":"y8ALmsAdn98I"},"outputs":[],"source":["def load_data_wiki(batch_size,max_len):\n","  num_workers = d2l.get_dataloader_workers()\n","  data_dir = d2l.download_extract('wikitext-2', 'wikitext-2')\n","  paragraphs = _read_wiki(data_dir)\n","  train_set = _WikiTextDataset(paragraphs,max_len)\n","  train_iter = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n","  return train_iter, train_set, train_set.vocab\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"rJYpN2_tzMJ-"},"source":["# build model"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"jgRFBoYbzQWP"},"source":["## 1.embedding"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1684943812355,"user":{"displayName":"shuaiqi duan","userId":"08897309869639517662"},"user_tz":-480},"id":"ChhRlKAozSVr"},"outputs":[],"source":["class bertEmbedding(nn.Module):\n","  def __init__(self, config, *args, **kwargs):\n","    super().__init__(*args, **kwargs)\n","    self.wordEmbedding = nn.Embedding(config.vocab_size,config.hidden_size)\n","    self.token_type_Embedding = nn.Embedding(config.type_vocab_size, config.hidden_size)\n","    self.position_Embedding = nn. Embedding(config.max_position_embeddings, config.hidden_size)\n","    \n","    self.position_ids = torch.arange(config.max_position_embeddings).expand((1,-1)).to(device)\n","\n","    self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n","    self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","    \n","\n","  def forward(self, input_ids, token_type_ids):\n","    wordEmbedding = self.wordEmbedding(input_ids)\n","    token_type_Embedding = self.token_type_Embedding(token_type_ids)\n","    position_embedding = self.position_Embedding(self.position_ids)\n","\n","    embedding = wordEmbedding + token_type_Embedding + position_embedding\n","\n","    embedding = self.LayerNorm(embedding)\n","\n","    embedding = self.dropout(embedding)\n","\n","    return embedding\n","   "]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"GZH-A4tcFOP-"},"source":["## 2.encoder"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"h3jMB5eiFQ4_"},"source":["### selfAttention"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1684943816719,"user":{"displayName":"shuaiqi duan","userId":"08897309869639517662"},"user_tz":-480},"id":"UvlRSQ9DFSxw"},"outputs":[],"source":["class bertSelfAttention(nn.Module):\n","  def __init__(self, config, *args, **kwargs):\n","    super().__init__(*args, **kwargs)\n","    self.num_attention_heads = config.num_attention_heads\n","    self.attention_head_size = int(config.hidden_size/config.num_attention_heads)\n","    self.all_head_size = self.num_attention_heads * self.attention_head_size\n","\n","    self.query = nn.Linear(config.hidden_size, self.all_head_size)\n","    self.key = nn.Linear(config.hidden_size, self.all_head_size)\n","    self.value = nn.Linear(config.hidden_size, self.all_head_size)\n","\n","    self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n","\n","  def transpose_for_scores(self, x:torch.Tensor) -> torch.Tensor:\n","    '''\n","    shape 64 * 128 * 768 -->64 * 128 * 12 * 64 --> 64 * 12 * 128 *64\n","    '''\n","    new_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n","    x = x.view(new_shape)\n","    x = x.permute(0,2,1,3)\n","    return x \n","\n","  def forward(self, hidden_states:torch.Tensor, attention_mask:torch.Tensor, output_attention: Optional[bool] = False) -> Tuple[torch.Tensor]:\n","\n","    query_layer = self.query(hidden_states)\n","    key_layer = self.key(hidden_states)\n","    value_layer = self.value(hidden_states)\n","\n","    query_layer = self.transpose_for_scores(query_layer)# 64 * 12 * 128 *64\n","    key_layer = self.transpose_for_scores(key_layer)# 64 * 12 * 128 *64\n","    value_layer = self.transpose_for_scores(value_layer)# 64 * 12 * 128 *64\n","\n","    attention_scores = torch.matmul(query_layer,key_layer.transpose(2,3))/math.sqrt(config.hidden_size)\n","    #64 * 12 * 128 * 128 \n","    #attention_mask 64 * 128\n","    mask_scores = torch.zeros(attention_mask.size(0), self.num_attention_heads, attention_mask.size(1),  attention_mask.size(1)).to(device)\n","    mask_scores += attention_mask[:,None,None,:]#自动广播\n","    mask_scores = (1 - mask_scores)* torch.finfo().min\n","\n","    attention_scores = attention_scores + mask_scores\n","\n","    attention_probs = nn.functional.softmax(attention_scores, dim = -1)\n","    attention_probs = self.dropout(attention_probs)\n","\n","    context_layer = torch.matmul(attention_probs, value_layer) # 64 * 12 * 128 *64\n","\n","    context_layer = context_layer.permute(0,2,1,3).contiguous().view(hidden_states.size(0),hidden_states.size(1),-1)\n","    \n","    outputs = (context_layer, attention_probs) if output_attention else (context_layer,)\n","\n","    return outputs\n"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1684943818377,"user":{"displayName":"shuaiqi duan","userId":"08897309869639517662"},"user_tz":-480},"id":"NE4T1Q00RGRQ"},"outputs":[],"source":["class bertSelfOutput(nn.Module):\n","  def __init__(self, config, *args, **kwargs):\n","    super().__init__(*args, **kwargs)\n","    self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n","    self.LayerNorm = nn.LayerNorm(config.hidden_size,eps=config.layer_norm_eps)\n","    self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","\n","  def forward(self, hidden_states:torch.Tensor, input_tensor:torch.Tensor)-> torch.Tensor:\n","    hidden_states = self.dense(hidden_states)\n","    hidden_states = self.dropout(hidden_states)\n","    hidden_states = self.LayerNorm(hidden_states + input_tensor)\n","    return hidden_states"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1684943820220,"user":{"displayName":"shuaiqi duan","userId":"08897309869639517662"},"user_tz":-480},"id":"70L1CKhoRHcw"},"outputs":[],"source":["class bertAttention(nn.Module):\n","  def __init__(self, config, *args, **kwargs):\n","    super().__init__(*args, **kwargs)\n","    self.selfAttention = bertSelfAttention(config)\n","    self.output = bertSelfOutput(config)\n","\n","  def forward(self,hidden_states:torch.Tensor, attention_mask:torch.Tensor, output_attention:Optional[bool] = False) -> Tuple[torch.Tensor]:\n","    selfoutput = self.selfAttention(hidden_states, attention_mask, output_attention)\n","    outputs = self.output(selfoutput[0], hidden_states)\n","    outputs = (outputs,) + selfoutput[1:]\n","    return outputs\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"UIpPje5zcltn"},"source":["### FFN"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1684943822925,"user":{"displayName":"shuaiqi duan","userId":"08897309869639517662"},"user_tz":-480},"id":"QBECrCOvcnKV"},"outputs":[],"source":["class bertIntermediate(nn.Module):\n","  def __init__(self, config, *args, **kwargs):\n","    super().__init__(*args, **kwargs)\n","    self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n","    self.act = nn.GELU()\n","\n","  def forward(self,hidden_states:torch.Tensor) ->torch.Tensor:\n","    hidden_states = self.dense(hidden_states)\n","    hidden_states = self.act(hidden_states)\n","    return hidden_states"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1684943824937,"user":{"displayName":"shuaiqi duan","userId":"08897309869639517662"},"user_tz":-480},"id":"s5ctrSV0eHTz"},"outputs":[],"source":["class InterOutput(nn.Module):\n","  def __init__(self, config, *args, **kwargs):\n","    super().__init__(*args, **kwargs)\n","    self.dense = nn.Linear(config.intermediate_size,config.hidden_size)\n","    self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n","    self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","\n","  def forward(self,hidden_states:torch.Tensor, input_tensor:torch.Tensor)->torch.Tensor:\n","    hidden_states = self.dense(hidden_states)\n","    hidden_states = self.dropout(hidden_states)\n","    hidden_states = self.LayerNorm(hidden_states + input_tensor)\n","    return hidden_states"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":771,"status":"ok","timestamp":1684943827402,"user":{"displayName":"shuaiqi duan","userId":"08897309869639517662"},"user_tz":-480},"id":"Qdh5CZbIgCOW"},"outputs":[],"source":["class FFN(nn.Module):\n","  def __init__(self, config, *args, **kwargs):\n","    super().__init__(*args, **kwargs)\n","    self.Intermediate = bertIntermediate(config)\n","    self.InterOutput = InterOutput(config)\n","\n","  def forward(self, hidden_states:torch.Tensor) -> torch.Tensor:\n","    Intermediate = self.Intermediate(hidden_states)\n","    FFNoutput = self.InterOutput(Intermediate, hidden_states)\n","    return FFNoutput"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"P-xybJwji8z6"},"source":["### bertLayer"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1684943829853,"user":{"displayName":"shuaiqi duan","userId":"08897309869639517662"},"user_tz":-480},"id":"_IvuxQNgi-bP"},"outputs":[],"source":["class bertLayer(nn.Module):\n","  def __init__(self, config, *args, **kwargs):\n","    super().__init__(*args, **kwargs)\n","    self.attention = bertAttention(config)\n","    self.FFN = FFN(config)\n","\n","  def forward(self, hidden_states:torch.Tensor, attention_mask:torch.Tensor, output_attention:Optional[bool] = False) -> Tuple[torch.Tensor]:\n","    self_attention_output = self.attention(\n","        hidden_states,\n","        attention_mask,\n","        output_attention\n","    )\n","    \n","    attention_output_state = self_attention_output[0]\n","    attention_output_prob = self_attention_output[1:]\n","\n","    layer_output = self.FFN(attention_output_state)\n","\n","    outputs = (layer_output,) + attention_output_prob\n","    return outputs "]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"vGXsYCYhrwq6"},"source":["### encoder"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1684943832815,"user":{"displayName":"shuaiqi duan","userId":"08897309869639517662"},"user_tz":-480},"id":"NRmfIaB1sI7q"},"outputs":[],"source":["class bertEncoder(nn.Module):\n","  def __init__(self, config, *args, **kwargs):\n","    super().__init__(*args, **kwargs)\n","    self.config = config\n","    self.layer = nn.ModuleList([bertLayer(config) for _ in range(config.num_hidden_layers)])\n","\n","\n","\n","  def forward(self,\n","              hidden_states:torch.Tensor,\n","              attention_mask:torch.Tensor,\n","              output_attentions: Optional[bool] = False,\n","              output_hidden_states: Optional[bool] = False\n","              )-> Tuple[torch.Tensor]:\n","\n","\n","              all_hidden_states = () if output_hidden_states else None\n","              all_self_attentions = () if output_attentions else None\n","\n","\n","              for i ,layer_module in enumerate(self.layer):\n","                if output_hidden_states:\n","                  all_hidden_states = all_hidden_states + (hidden_states,)\n","\n","\n","                layer_outputs = layer_module(\n","                    hidden_states,\n","                    attention_mask,\n","                    output_attentions\n","                )\n","                hidden_states = layer_outputs[0]\n","\n","                if output_attentions:\n","                  all_self_attentions = all_self_attentions + (layer_outputs[1],)\n","\n","              if output_hidden_states:\n","                all_hidden_states = all_hidden_states + (hidden_states,)\n","              \n","              return tuple(v for v in [hidden_states, all_hidden_states, all_self_attentions] if v is not None)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"u9EjldBKy5qI"},"source":["## 3.pooler"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":336,"status":"ok","timestamp":1684943835586,"user":{"displayName":"shuaiqi duan","userId":"08897309869639517662"},"user_tz":-480},"id":"SJwyO5pgy7U1"},"outputs":[],"source":["class bertPooler(nn.Module):\n","  def __init__(self, config, *args, **kwargs):\n","    super().__init__(*args, **kwargs)\n","    self.dense = nn.Linear(config.hidden_size,config.hidden_size)\n","    self.act = nn.Tanh()\n","\n","\n","  def forward(self, hidden_states:torch.Tensor) -> torch.Tensor:\n","      pooloutput = self.dense(hidden_states[:,0])\n","      pooloutput = self.act(pooloutput)\n","      return pooloutput\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"taL8vXNt07p5"},"source":["## 4.bertmodel"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1684943839631,"user":{"displayName":"shuaiqi duan","userId":"08897309869639517662"},"user_tz":-480},"id":"Mgl_pcGi090F"},"outputs":[],"source":["class bertmodel(nn.Module):\n","  def __init__(self, config, *args, **kwargs):\n","    super().__init__(*args, **kwargs)\n","    self.config = config\n","    self.embedding = bertEmbedding(config)\n","    self.encoder = bertEncoder(config)\n","    self.pool = bertPooler(config)\n","\n","\n","  def forward(self, input_ids:torch.Tensor, token_type_ids:torch.Tensor, attention_mask:torch.Tensor, output_attentions: Optional[bool] = False,\n","              output_hidden_states: Optional[bool] = False)->tuple[torch.Tensor]:\n","\n","              embedding_output = self.embedding(input_ids,token_type_ids)\n","              encoder_output = self.encoder(embedding_output, attention_mask, output_attentions = output_attentions, output_hidden_states = output_hidden_states)\n","              sequence_output = encoder_output[0]\n","              pooled_output = self.pool(sequence_output)\n","\n","              return (sequence_output, pooled_output,) + encoder_output[1:]"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"QO3aEHwBockR"},"source":["# pretrained task"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Z5JzNahaokHb"},"source":["## 1.mask language model"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1684943843570,"user":{"displayName":"shuaiqi duan","userId":"08897309869639517662"},"user_tz":-480},"id":"f9aFDBIOop7T"},"outputs":[],"source":["class MaskLM(nn.Module):\n","  def __init__(self, config,  *args, **kwargs):\n","    super().__init__(*args, **kwargs)\n","    self.mlp = nn.Sequential(\n","        nn.Linear(config.hidden_size, config.hidden_size),\n","        nn.ReLU(),\n","        nn.LayerNorm(config.hidden_size,eps=config.layer_norm_eps),\n","        nn.Linear(config.hidden_size,config.vocab_size)\n","    )\n","\n","  def forward(self, encoder_states:torch.Tensor, pred_positions:torch.Tensor)->torch.Tensor:\n","      num_pred_positions = pred_positions.shape[1] #19\n","      pred_positions = pred_positions.view(-1)# 64*19的向量\n","      batch_size = encoder_states.shape[0] #64\n","      batch_idx = torch.arange(0, batch_size) #19\n","      '''\n","      这里是要实现这样一个事情，把num_pred_positions这个64*19的tensor拉成一个向量\n","      这个向量的前19个元素是第一个batch里的预测的位置，之后是19个是第二个batch里的预测位置 。。。。\n","      但是现在的batch_idx是这样的一个tensor([0,0,0,...,0,1,1,....,1,2,2,......])\n","      '''\n","      batch_idx = torch.repeat_interleave(batch_idx, num_pred_positions)\n","      masked = encoder_states[batch_idx, pred_positions]\n","      masked = masked.view((batch_size, num_pred_positions,-1))\n","      mlm_pred = self.mlp(masked)\n","      return mlm_pred\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"lo5fv1nx5dWK"},"source":["## 2.next sentence predict"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1684943845931,"user":{"displayName":"shuaiqi duan","userId":"08897309869639517662"},"user_tz":-480},"id":"s6U__Jn-7Ok1"},"outputs":[],"source":["class NextSentencePred(nn.Module):\n","  def __init__(self, config,  *args, **kwargs):\n","    super().__init__(*args, **kwargs)\n","    self.output = nn.Linear(config.hidden_size, 2)\n","\n","  def forward(self, pooler_output:torch.Tensor)->torch.Tensor:\n","    '''\n","    pooler_output 为 64 * 768 的tensor\n","    '''\n","    return self.output(pooler_output)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"BvFv7v648zuZ"},"source":["# bertLanguageModel\n"]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1684943848318,"user":{"displayName":"shuaiqi duan","userId":"08897309869639517662"},"user_tz":-480},"id":"1zVV_1XI83rE"},"outputs":[],"source":["class bertLanguageModel(nn.Module):\n","  def __init__(self, config,  *args, **kwargs):\n","    super().__init__(*args, **kwargs)\n","\n","    self.bert = bertmodel(config)\n","    # 输出encoder_states 64*128*768 和 pooler 64*768\n","    self.mlm = MaskLM(config)\n","    #输出 64*19*vocab_size\n","    self.nsp = NextSentencePred(config)\n","    #输出 64*2\n","\n","  def forward(self, input_ids:torch.Tensor, token_type_ids:torch.Tensor, attention_mask:torch.Tensor,  pred_positions:torch.Tensor, output_attentions: Optional[bool] = False,\n","              output_hidden_states: Optional[bool] = False) -> tuple[torch.Tensor]:\n","    encoder_states = self.bert(input_ids, token_type_ids, attention_mask, output_attentions, output_hidden_states)\n","    encoder_seq = encoder_states[0]\n","    pooler_output = encoder_states[1]\n","    mlm_pred = self.mlm(encoder_seq, pred_positions)\n","    nsp_pred = self.nsp(pooler_output)\n","\n","    return (encoder_seq, mlm_pred, nsp_pred)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"n1VyYmX-AuCY"},"source":["# train"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"8A9f1e1-BdNs"},"source":["## loss function"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1684943851856,"user":{"displayName":"shuaiqi duan","userId":"08897309869639517662"},"user_tz":-480},"id":"NJyqOG2wBibi"},"outputs":[],"source":["def _get_batch_loss(loss, mlm_pred, nsp_pred, mlm_weights, mlm_l, nsp_l, config):\n","  '''\n","  mlm_pred 64*19*vocab_size\n","  mlm_l 64*19\n","  mlm_weights 64*19\n","  nsp_pred 64*2\n","  nsp_l 64\n","  注意我们这里使用的交叉熵reduction参数要加none 不能自动求均值，因为mask还有padding需要处理\n","  加了参数reduction='none'后loss不会自动求均值，而是会返回每个位置的损失，返回一个tensor向量，大小为64*19\n","  这时，将mlm_weights view成一个64*19的一维tensor也就是向量，做向量点乘(不是内积)，返回一个64*19的向量，这时这个向量\n","  的padding部分已经和0相乘变成0，在对tensor求和求均值\n","  或者可以直接用torch.matmul() 如果传入两个tensor都是一维向量，则会做向量内积，返回一个标量 scaler 这个就是对padding处理后的真实损失，直接求均值即可\n","  '''\n","\n","  mlm_loss = loss(mlm_pred.view(-1,config.vocab_size), mlm_l.view(-1)) * mlm_weights.view(-1)  #mask掉padding的预测位置\n","  mlm_loss = mlm_loss.sum() / (mlm_weights.sum() + 1e-8) #加1e-8 做偏差修正，防止除数为0，常规操作\n","\n","  nsp_loss = loss(nsp_pred,nsp_l)\n","  nsp_loss = nsp_loss.sum() / (nsp_loss.shape[0]) \n","\n","  loss = nsp_loss + mlm_loss\n","  return (loss, nsp_loss, mlm_loss)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"AGkUKHPLRrYh"},"source":["## train function"]},{"cell_type":"code","execution_count":149,"metadata":{"executionInfo":{"elapsed":352,"status":"ok","timestamp":1684944096807,"user":{"displayName":"shuaiqi duan","userId":"08897309869639517662"},"user_tz":-480},"id":"eSHsK01AAFrG"},"outputs":[],"source":["def train_bert(dataloader, model, optim, loss, config, device):\n","  mlm_loss_sum = []\n","  nsp_loss_sum = [] \n","  sentence_pair_sum = []\n","  batch_sum = [] \n","  total_time = [] \n","  \n","\n","  for epoch in range(config.epochs):\n","    fig, ax = plt.subplots()\n","    mlm_loss_current_epoch = []\n","    nsp_loss_current_epoch = []\n","    sentence_pair_current_epoch = 0\n","    batch_current_epoch = 0\n","    total_time_current_epoch = 0\n","    with open(f'./{epoch}.txt', 'w') as f:\n","      for input_ids, token_type_ids, attention_mask, pred_positions, mlm_weights, mlm_l, nsp_l in dataloader:\n","        optim.zero_grad()\n","        start_time = time.time()\n","        encoder_states = model(input_ids, token_type_ids, attention_mask, pred_positions)\n","        mlm_pred = encoder_states[1]\n","        nsp_pred = encoder_states[2]\n","        loss_ = _get_batch_loss(loss, mlm_pred, nsp_pred, mlm_weights, mlm_l, nsp_l, config)\n","        mlm_loss = loss_[2]\n","        nsp_loss = loss_[1]\n","        loss_sum = loss_[0]\n","        loss_sum.backward()\n","        optim.step()\n","        mlm_loss_current_epoch.append(mlm_loss.cpu().detach().numpy())\n","        nsp_loss_current_epoch.append(nsp_loss.cpu().detach().numpy())\n","        sentence_pair_current_epoch += input_ids.shape[0]\n","        batch_current_epoch += 1\n","        end_time = time.time()\n","        current_batch_time = end_time - start_time\n","        total_time_current_epoch += current_batch_time\n","        ax.cla()\n","        ax.plot(mlm_loss_current_epoch, label='mlm_loss')\n","        ax.plot(nsp_loss_current_epoch, label='nsp_loss')\n","        ax.legend()\n","        ax.set_title(f'epoch {epoch} Losses')\n","        ax.set_xlabel('Batch')\n","        ax.set_ylabel('Loss')\n","        ax.set_xlim(0, len(dataloader))\n","        display.display(fig)\n","        display.clear_output(wait=True)\n","      \n","      \n","        log = f'currrent_batch_mlm_loss {mlm_loss:.3f}  ' f'currrent_batch_nsp_loss {nsp_loss:.3f}  ' \\\n","              f'{input_ids.shape[0] / current_batch_time:.2f}  '  'sentence pairs /sec on  ' f'{str(device)}  '  \\\n","              f'{current_batch_time:2f}  '  ' sec elapsed in current batch' f'{batch_current_epoch}  ' + '\\n'\n","        f.write(log)\n","     \n","  \n","      plt.savefig(f'./{epoch}.jpg')\n","      plt.close() \n","    \n","    \n","      \n","      log = f'epoch {epoch} MLM LOSS {sum(mlm_loss_current_epoch) / batch_current_epoch:.3f}  ' \\\n","            f'NSP LOSS {sum(nsp_loss_current_epoch) / batch_current_epoch:.3f}  ' \\\n","            f'{sentence_pair_current_epoch / total_time_current_epoch:.1f}  '   'sentences pairs/sec on  ' f'{str(device)}  ' \\\n","            f'{total_time_current_epoch:2f} sec elapsed in current epoch  ' f'{epoch}  ' + '\\n'      \n","      f.write(log)\n","      f.close()\n","    mlm_loss_sum.append(mlm_loss_current_epoch)\n","    nsp_loss_sum.append(nsp_loss_current_epoch)\n","    sentence_pair_sum.append(sentence_pair_current_epoch)\n","    batch_sum.append(batch_current_epoch)\n","    total_time.append(total_time_current_epoch)\n","    "]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"CgPUs5QOR3Q2"},"source":["# begin train"]},{"cell_type":"code","execution_count":93,"metadata":{"executionInfo":{"elapsed":751,"status":"ok","timestamp":1684943857401,"user":{"displayName":"shuaiqi duan","userId":"08897309869639517662"},"user_tz":-480},"id":"2FcfywH9UCGI"},"outputs":[],"source":["class config():\n","  def __init__(self):\n","    self.vocab_size = 20256\n","    self.hidden_size = 768\n","    self.type_vocab_size = 2\n","    self.max_position_embeddings = 128\n","    self.layer_norm_eps = 1e-12\n","    self.hidden_dropout_prob = 0.1\n","    self.num_attention_heads = 12\n","    self.attention_probs_dropout_prob = 0.1\n","    self.intermediate_size = 3072\n","    self.num_hidden_layers = 12\n","    self.batch_size = 64\n","    self.max_len = 128\n","    self.epochs = 5\n","\n","config = config()"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25372,"status":"ok","timestamp":1684943884932,"user":{"displayName":"shuaiqi duan","userId":"08897309869639517662"},"user_tz":-480},"id":"ujvVXrjuR6A1","outputId":"73fc8761-6b74-4b48-ae1d-6b6ff78cc643"},"outputs":[],"source":["Dataloader, Dataset, vocab = load_data_wiki(config.batch_size,config.max_len)"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["model = bertLanguageModel(config).to(device)"]},{"cell_type":"code","execution_count":31,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1684943894641,"user":{"displayName":"shuaiqi duan","userId":"08897309869639517662"},"user_tz":-480},"id":"XOU8IcTLYbai"},"outputs":[],"source":["loss = nn.CrossEntropyLoss(reduction='none')\n","optim = torch.optim.AdamW(model.parameters(), amsgrad=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":265},"id":"rAig3m6dY5af","outputId":"5f3be5ea-9421-431f-b5cd-bd829e6fccc2"},"outputs":[],"source":["train_bert(Dataloader, model, optim, loss, config, device)"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNSx7nsoTaGfwBR3iId64el","gpuType":"V100","machine_shape":"hm","mount_file_id":"1HgeOj25FFjOvXmrh5XWbCHmqY3fDHbK0","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"}},"nbformat":4,"nbformat_minor":0}
