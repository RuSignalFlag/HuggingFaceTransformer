{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15071,"status":"ok","timestamp":1684242167575,"user":{"displayName":"shuaiqi duan","userId":"08897309869639517662"},"user_tz":-480},"id":"eNmS0YASp1kv","outputId":"427c2994-2591-4e21-8b4c-fa8af863fdd4"},"outputs":[],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":341,"status":"ok","timestamp":1684242532826,"user":{"displayName":"shuaiqi duan","userId":"08897309869639517662"},"user_tz":-480},"id":"_vI8ZZNEp49c"},"outputs":[],"source":["from transformers import BertModel, BertForSequenceClassification"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4195,"status":"ok","timestamp":1684242582961,"user":{"displayName":"shuaiqi duan","userId":"08897309869639517662"},"user_tz":-480},"id":"JmIAUCvSqE4m","outputId":"c40d61a1-c4ea-438e-e321-5360a2fbb166"},"outputs":[],"source":["model = BertModel.from_pretrained('bert-base-uncased')\n","cls_model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":345,"status":"ok","timestamp":1684242589543,"user":{"displayName":"shuaiqi duan","userId":"08897309869639517662"},"user_tz":-480},"id":"fB5tgMy0qUpP","outputId":"2e1d28d4-d35e-45aa-a79c-bab046cf6837"},"outputs":[{"data":{"text/plain":["BertModel(\n","  (embeddings): BertEmbeddings(\n","    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","    (position_embeddings): Embedding(512, 768)\n","    (token_type_embeddings): Embedding(2, 768)\n","    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (encoder): BertEncoder(\n","    (layer): ModuleList(\n","      (0-11): 12 x BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","  )\n","  (pooler): BertPooler(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (activation): Tanh()\n","  )\n",")"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["model"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1684242599221,"user":{"displayName":"shuaiqi duan","userId":"08897309869639517662"},"user_tz":-480},"id":"uSoYQhzlqYUQ","outputId":"bb3414f2-459e-44ad-d40e-7f2fad13611f"},"outputs":[{"data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["cls_model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uPImWyPFroUl"},"outputs":[],"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"5HbiU_oHr1Sl"},"source":["### summary\n","1.bert is an encoder of transformer\n","\n","2.embedding\n","\n","3.encoder 12 layers\n","\n","4.pooler\n","\n","\n","   "]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1684243084186,"user":{"displayName":"shuaiqi duan","userId":"08897309869639517662"},"user_tz":-480},"id":"KXzKfkMxtGfN"},"outputs":[],"source":["#参数量统计\n","model_params=0\n","model_learnable_params=0"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ze7zhH4YtHIG"},"source":["# "]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1684243540439,"user":{"displayName":"shuaiqi duan","userId":"08897309869639517662"},"user_tz":-480},"id":"Ve_9MvzwtfaT","outputId":"4a1b11cd-e35e-453c-a8fc-08cd41f4bd44"},"outputs":[{"name":"stdout","output_type":"stream","text":["embeddings.word_embeddings.weight torch.Size([30522, 768])\n","embeddings.position_embeddings.weight torch.Size([512, 768])\n","embeddings.token_type_embeddings.weight torch.Size([2, 768])\n","embeddings.LayerNorm.weight torch.Size([768])\n","embeddings.LayerNorm.bias torch.Size([768])\n","encoder.layer.0.attention.self.query.weight torch.Size([768, 768])\n","encoder.layer.0.attention.self.query.bias torch.Size([768])\n","encoder.layer.0.attention.self.key.weight torch.Size([768, 768])\n","encoder.layer.0.attention.self.key.bias torch.Size([768])\n","encoder.layer.0.attention.self.value.weight torch.Size([768, 768])\n","encoder.layer.0.attention.self.value.bias torch.Size([768])\n","encoder.layer.0.attention.output.dense.weight torch.Size([768, 768])\n","encoder.layer.0.attention.output.dense.bias torch.Size([768])\n","encoder.layer.0.attention.output.LayerNorm.weight torch.Size([768])\n","encoder.layer.0.attention.output.LayerNorm.bias torch.Size([768])\n","encoder.layer.0.intermediate.dense.weight torch.Size([3072, 768])\n","encoder.layer.0.intermediate.dense.bias torch.Size([3072])\n","encoder.layer.0.output.dense.weight torch.Size([768, 3072])\n","encoder.layer.0.output.dense.bias torch.Size([768])\n","encoder.layer.0.output.LayerNorm.weight torch.Size([768])\n","encoder.layer.0.output.LayerNorm.bias torch.Size([768])\n","encoder.layer.1.attention.self.query.weight torch.Size([768, 768])\n","encoder.layer.1.attention.self.query.bias torch.Size([768])\n","encoder.layer.1.attention.self.key.weight torch.Size([768, 768])\n","encoder.layer.1.attention.self.key.bias torch.Size([768])\n","encoder.layer.1.attention.self.value.weight torch.Size([768, 768])\n","encoder.layer.1.attention.self.value.bias torch.Size([768])\n","encoder.layer.1.attention.output.dense.weight torch.Size([768, 768])\n","encoder.layer.1.attention.output.dense.bias torch.Size([768])\n","encoder.layer.1.attention.output.LayerNorm.weight torch.Size([768])\n","encoder.layer.1.attention.output.LayerNorm.bias torch.Size([768])\n","encoder.layer.1.intermediate.dense.weight torch.Size([3072, 768])\n","encoder.layer.1.intermediate.dense.bias torch.Size([3072])\n","encoder.layer.1.output.dense.weight torch.Size([768, 3072])\n","encoder.layer.1.output.dense.bias torch.Size([768])\n","encoder.layer.1.output.LayerNorm.weight torch.Size([768])\n","encoder.layer.1.output.LayerNorm.bias torch.Size([768])\n","encoder.layer.2.attention.self.query.weight torch.Size([768, 768])\n","encoder.layer.2.attention.self.query.bias torch.Size([768])\n","encoder.layer.2.attention.self.key.weight torch.Size([768, 768])\n","encoder.layer.2.attention.self.key.bias torch.Size([768])\n","encoder.layer.2.attention.self.value.weight torch.Size([768, 768])\n","encoder.layer.2.attention.self.value.bias torch.Size([768])\n","encoder.layer.2.attention.output.dense.weight torch.Size([768, 768])\n","encoder.layer.2.attention.output.dense.bias torch.Size([768])\n","encoder.layer.2.attention.output.LayerNorm.weight torch.Size([768])\n","encoder.layer.2.attention.output.LayerNorm.bias torch.Size([768])\n","encoder.layer.2.intermediate.dense.weight torch.Size([3072, 768])\n","encoder.layer.2.intermediate.dense.bias torch.Size([3072])\n","encoder.layer.2.output.dense.weight torch.Size([768, 3072])\n","encoder.layer.2.output.dense.bias torch.Size([768])\n","encoder.layer.2.output.LayerNorm.weight torch.Size([768])\n","encoder.layer.2.output.LayerNorm.bias torch.Size([768])\n","encoder.layer.3.attention.self.query.weight torch.Size([768, 768])\n","encoder.layer.3.attention.self.query.bias torch.Size([768])\n","encoder.layer.3.attention.self.key.weight torch.Size([768, 768])\n","encoder.layer.3.attention.self.key.bias torch.Size([768])\n","encoder.layer.3.attention.self.value.weight torch.Size([768, 768])\n","encoder.layer.3.attention.self.value.bias torch.Size([768])\n","encoder.layer.3.attention.output.dense.weight torch.Size([768, 768])\n","encoder.layer.3.attention.output.dense.bias torch.Size([768])\n","encoder.layer.3.attention.output.LayerNorm.weight torch.Size([768])\n","encoder.layer.3.attention.output.LayerNorm.bias torch.Size([768])\n","encoder.layer.3.intermediate.dense.weight torch.Size([3072, 768])\n","encoder.layer.3.intermediate.dense.bias torch.Size([3072])\n","encoder.layer.3.output.dense.weight torch.Size([768, 3072])\n","encoder.layer.3.output.dense.bias torch.Size([768])\n","encoder.layer.3.output.LayerNorm.weight torch.Size([768])\n","encoder.layer.3.output.LayerNorm.bias torch.Size([768])\n","encoder.layer.4.attention.self.query.weight torch.Size([768, 768])\n","encoder.layer.4.attention.self.query.bias torch.Size([768])\n","encoder.layer.4.attention.self.key.weight torch.Size([768, 768])\n","encoder.layer.4.attention.self.key.bias torch.Size([768])\n","encoder.layer.4.attention.self.value.weight torch.Size([768, 768])\n","encoder.layer.4.attention.self.value.bias torch.Size([768])\n","encoder.layer.4.attention.output.dense.weight torch.Size([768, 768])\n","encoder.layer.4.attention.output.dense.bias torch.Size([768])\n","encoder.layer.4.attention.output.LayerNorm.weight torch.Size([768])\n","encoder.layer.4.attention.output.LayerNorm.bias torch.Size([768])\n","encoder.layer.4.intermediate.dense.weight torch.Size([3072, 768])\n","encoder.layer.4.intermediate.dense.bias torch.Size([3072])\n","encoder.layer.4.output.dense.weight torch.Size([768, 3072])\n","encoder.layer.4.output.dense.bias torch.Size([768])\n","encoder.layer.4.output.LayerNorm.weight torch.Size([768])\n","encoder.layer.4.output.LayerNorm.bias torch.Size([768])\n","encoder.layer.5.attention.self.query.weight torch.Size([768, 768])\n","encoder.layer.5.attention.self.query.bias torch.Size([768])\n","encoder.layer.5.attention.self.key.weight torch.Size([768, 768])\n","encoder.layer.5.attention.self.key.bias torch.Size([768])\n","encoder.layer.5.attention.self.value.weight torch.Size([768, 768])\n","encoder.layer.5.attention.self.value.bias torch.Size([768])\n","encoder.layer.5.attention.output.dense.weight torch.Size([768, 768])\n","encoder.layer.5.attention.output.dense.bias torch.Size([768])\n","encoder.layer.5.attention.output.LayerNorm.weight torch.Size([768])\n","encoder.layer.5.attention.output.LayerNorm.bias torch.Size([768])\n","encoder.layer.5.intermediate.dense.weight torch.Size([3072, 768])\n","encoder.layer.5.intermediate.dense.bias torch.Size([3072])\n","encoder.layer.5.output.dense.weight torch.Size([768, 3072])\n","encoder.layer.5.output.dense.bias torch.Size([768])\n","encoder.layer.5.output.LayerNorm.weight torch.Size([768])\n","encoder.layer.5.output.LayerNorm.bias torch.Size([768])\n","encoder.layer.6.attention.self.query.weight torch.Size([768, 768])\n","encoder.layer.6.attention.self.query.bias torch.Size([768])\n","encoder.layer.6.attention.self.key.weight torch.Size([768, 768])\n","encoder.layer.6.attention.self.key.bias torch.Size([768])\n","encoder.layer.6.attention.self.value.weight torch.Size([768, 768])\n","encoder.layer.6.attention.self.value.bias torch.Size([768])\n","encoder.layer.6.attention.output.dense.weight torch.Size([768, 768])\n","encoder.layer.6.attention.output.dense.bias torch.Size([768])\n","encoder.layer.6.attention.output.LayerNorm.weight torch.Size([768])\n","encoder.layer.6.attention.output.LayerNorm.bias torch.Size([768])\n","encoder.layer.6.intermediate.dense.weight torch.Size([3072, 768])\n","encoder.layer.6.intermediate.dense.bias torch.Size([3072])\n","encoder.layer.6.output.dense.weight torch.Size([768, 3072])\n","encoder.layer.6.output.dense.bias torch.Size([768])\n","encoder.layer.6.output.LayerNorm.weight torch.Size([768])\n","encoder.layer.6.output.LayerNorm.bias torch.Size([768])\n","encoder.layer.7.attention.self.query.weight torch.Size([768, 768])\n","encoder.layer.7.attention.self.query.bias torch.Size([768])\n","encoder.layer.7.attention.self.key.weight torch.Size([768, 768])\n","encoder.layer.7.attention.self.key.bias torch.Size([768])\n","encoder.layer.7.attention.self.value.weight torch.Size([768, 768])\n","encoder.layer.7.attention.self.value.bias torch.Size([768])\n","encoder.layer.7.attention.output.dense.weight torch.Size([768, 768])\n","encoder.layer.7.attention.output.dense.bias torch.Size([768])\n","encoder.layer.7.attention.output.LayerNorm.weight torch.Size([768])\n","encoder.layer.7.attention.output.LayerNorm.bias torch.Size([768])\n","encoder.layer.7.intermediate.dense.weight torch.Size([3072, 768])\n","encoder.layer.7.intermediate.dense.bias torch.Size([3072])\n","encoder.layer.7.output.dense.weight torch.Size([768, 3072])\n","encoder.layer.7.output.dense.bias torch.Size([768])\n","encoder.layer.7.output.LayerNorm.weight torch.Size([768])\n","encoder.layer.7.output.LayerNorm.bias torch.Size([768])\n","encoder.layer.8.attention.self.query.weight torch.Size([768, 768])\n","encoder.layer.8.attention.self.query.bias torch.Size([768])\n","encoder.layer.8.attention.self.key.weight torch.Size([768, 768])\n","encoder.layer.8.attention.self.key.bias torch.Size([768])\n","encoder.layer.8.attention.self.value.weight torch.Size([768, 768])\n","encoder.layer.8.attention.self.value.bias torch.Size([768])\n","encoder.layer.8.attention.output.dense.weight torch.Size([768, 768])\n","encoder.layer.8.attention.output.dense.bias torch.Size([768])\n","encoder.layer.8.attention.output.LayerNorm.weight torch.Size([768])\n","encoder.layer.8.attention.output.LayerNorm.bias torch.Size([768])\n","encoder.layer.8.intermediate.dense.weight torch.Size([3072, 768])\n","encoder.layer.8.intermediate.dense.bias torch.Size([3072])\n","encoder.layer.8.output.dense.weight torch.Size([768, 3072])\n","encoder.layer.8.output.dense.bias torch.Size([768])\n","encoder.layer.8.output.LayerNorm.weight torch.Size([768])\n","encoder.layer.8.output.LayerNorm.bias torch.Size([768])\n","encoder.layer.9.attention.self.query.weight torch.Size([768, 768])\n","encoder.layer.9.attention.self.query.bias torch.Size([768])\n","encoder.layer.9.attention.self.key.weight torch.Size([768, 768])\n","encoder.layer.9.attention.self.key.bias torch.Size([768])\n","encoder.layer.9.attention.self.value.weight torch.Size([768, 768])\n","encoder.layer.9.attention.self.value.bias torch.Size([768])\n","encoder.layer.9.attention.output.dense.weight torch.Size([768, 768])\n","encoder.layer.9.attention.output.dense.bias torch.Size([768])\n","encoder.layer.9.attention.output.LayerNorm.weight torch.Size([768])\n","encoder.layer.9.attention.output.LayerNorm.bias torch.Size([768])\n","encoder.layer.9.intermediate.dense.weight torch.Size([3072, 768])\n","encoder.layer.9.intermediate.dense.bias torch.Size([3072])\n","encoder.layer.9.output.dense.weight torch.Size([768, 3072])\n","encoder.layer.9.output.dense.bias torch.Size([768])\n","encoder.layer.9.output.LayerNorm.weight torch.Size([768])\n","encoder.layer.9.output.LayerNorm.bias torch.Size([768])\n","encoder.layer.10.attention.self.query.weight torch.Size([768, 768])\n","encoder.layer.10.attention.self.query.bias torch.Size([768])\n","encoder.layer.10.attention.self.key.weight torch.Size([768, 768])\n","encoder.layer.10.attention.self.key.bias torch.Size([768])\n","encoder.layer.10.attention.self.value.weight torch.Size([768, 768])\n","encoder.layer.10.attention.self.value.bias torch.Size([768])\n","encoder.layer.10.attention.output.dense.weight torch.Size([768, 768])\n","encoder.layer.10.attention.output.dense.bias torch.Size([768])\n","encoder.layer.10.attention.output.LayerNorm.weight torch.Size([768])\n","encoder.layer.10.attention.output.LayerNorm.bias torch.Size([768])\n","encoder.layer.10.intermediate.dense.weight torch.Size([3072, 768])\n","encoder.layer.10.intermediate.dense.bias torch.Size([3072])\n","encoder.layer.10.output.dense.weight torch.Size([768, 3072])\n","encoder.layer.10.output.dense.bias torch.Size([768])\n","encoder.layer.10.output.LayerNorm.weight torch.Size([768])\n","encoder.layer.10.output.LayerNorm.bias torch.Size([768])\n","encoder.layer.11.attention.self.query.weight torch.Size([768, 768])\n","encoder.layer.11.attention.self.query.bias torch.Size([768])\n","encoder.layer.11.attention.self.key.weight torch.Size([768, 768])\n","encoder.layer.11.attention.self.key.bias torch.Size([768])\n","encoder.layer.11.attention.self.value.weight torch.Size([768, 768])\n","encoder.layer.11.attention.self.value.bias torch.Size([768])\n","encoder.layer.11.attention.output.dense.weight torch.Size([768, 768])\n","encoder.layer.11.attention.output.dense.bias torch.Size([768])\n","encoder.layer.11.attention.output.LayerNorm.weight torch.Size([768])\n","encoder.layer.11.attention.output.LayerNorm.bias torch.Size([768])\n","encoder.layer.11.intermediate.dense.weight torch.Size([3072, 768])\n","encoder.layer.11.intermediate.dense.bias torch.Size([3072])\n","encoder.layer.11.output.dense.weight torch.Size([768, 3072])\n","encoder.layer.11.output.dense.bias torch.Size([768])\n","encoder.layer.11.output.LayerNorm.weight torch.Size([768])\n","encoder.layer.11.output.LayerNorm.bias torch.Size([768])\n","pooler.dense.weight torch.Size([768, 768])\n","pooler.dense.bias torch.Size([768])\n","109482240\n","109482240\n"]}],"source":["model_params=0\n","model_learnable_params=0\n","for name, params in model.named_parameters():\n","  print(name, params.shape)\n","  if params.requires_grad:\n","    model_learnable_params += params.numel()\n","  model_params += params.numel()\n","\n","print(model_params)\n","print(model_learnable_params)\n"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":1072,"status":"ok","timestamp":1684243812320,"user":{"displayName":"shuaiqi duan","userId":"08897309869639517662"},"user_tz":-480},"id":"v8vGGonZt1A2"},"outputs":[],"source":["embedding_params = 0\n","encoder_params = 0\n","pooler_params = 0\n","\n","for name,param in model.named_parameters():\n","  if 'embedding' in name:\n","    embedding_params += param.numel()\n","  if 'encoder' in name:\n","    encoder_params += param.numel()\n","  if 'pooler' in name:\n","    pooler_params += param.numel()\n","\n"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1684243824861,"user":{"displayName":"shuaiqi duan","userId":"08897309869639517662"},"user_tz":-480},"id":"skzU_jyPwQhh","outputId":"97104fb5-844a-4c5b-a7ab-b0e971e459ff"},"outputs":[{"data":{"text/plain":["23837184"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["embedding_params"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1684243832605,"user":{"displayName":"shuaiqi duan","userId":"08897309869639517662"},"user_tz":-480},"id":"VBV4hk7OwTBk","outputId":"5c440df8-74dc-43ab-a8a5-8ec5ba8b9b14"},"outputs":[{"data":{"text/plain":["85054464"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["encoder_params"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":321,"status":"ok","timestamp":1684243838072,"user":{"displayName":"shuaiqi duan","userId":"08897309869639517662"},"user_tz":-480},"id":"UDqLWV8EwVgV","outputId":"b4fb3fca-dd26-4b86-f41d-5a8590877137"},"outputs":[{"data":{"text/plain":["590592"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["pooler_params"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jexm0uPgwW7M"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOvDDoPd20P8FSOrxV+vidO","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
